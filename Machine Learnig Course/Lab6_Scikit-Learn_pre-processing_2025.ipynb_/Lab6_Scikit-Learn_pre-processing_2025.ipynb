{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5552bf23",
   "metadata": {},
   "source": [
    "# Lab 9: Pre-Processing with Scikit-Learn and Pandas\n",
    "\n",
    "The objective of this notebook is to learn about **pre-processing** with the **Scikit-Learn** and **Pandas** libraries. Then, train a simple binary classifier on the pre-processed dataset.\n",
    "\n",
    "In this lab, we will train a binary classification model that predicts which **passengers survived** the **Titanic shipwreck** <a href=\"https://www.kaggle.com/c/titanic\" >link</a>.\n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this notebook, you are asked to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc). For now we just focus on the preprocessing part, but you can come back here later after the laboratory on classification\n",
    "\n",
    "You can find a detailed **tutorial** <a href=\"https://datasciencewithchris.com/kaggle-titanic-data-cleaning-and-preprocessing/\" >here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39915670",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [1. Load Dataset](#1)\n",
    "- [2.  Data pre-processing](#2)\n",
    "- [3. Model training](#3) (Following the workshop on classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e004f3",
   "metadata": {},
   "source": [
    "First, run the following cell to import some useful libraries to complete this Lab. If not already done, you must install them in your virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffeedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns= 50\n",
    "pd.options.display.max_rows= None\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa4167",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Load dataset\n",
    "\n",
    "Firstly, you will load the **Titanic** dataset used in this lab into a DataFrame `df`. \n",
    "\n",
    "**Scikit-Learn** comes with built-in datasets for the **Titanic dataset**. The next cell loads the titanic dataset from Scikit-Learn and stores it in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8836fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, y = fetch_openml('titanic', version=1, as_frame=True, parser='auto', return_X_y=True)\n",
    "df[\"survived\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6139747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"lab6_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3a337",
   "metadata": {},
   "source": [
    "Run the next cell to look at the first 5 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c775e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ea053",
   "metadata": {},
   "source": [
    "The dataset is composed of 1309 samples. Each row contains information on each passenger. Specifically, the dataset contains the following attributes:\n",
    "\n",
    "- **pclass**: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "- **name**: Passenger name\n",
    "- **sex**: Passenger sex\n",
    "- **age**: Passenger age\n",
    "- **sibsp**: Number of Siblings/Spouses Aboard\n",
    "- **parch**: Number of Parents/Children Aboard\n",
    "- **ticket**: Ticket Number\n",
    "- **fare**: Passenger Fare\n",
    "- **cabin**: Cabin\n",
    "- **embarked**: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "- **boat**: Lifeboat (if survived)\n",
    "- **body**: Body number (if did not survive and body was recovered). It could be another target.\n",
    "- **home.dest**: Destination\n",
    "- **survival** (target): Survival (0 = No; 1 = Yes)\n",
    "\n",
    "Note that **boat** and **body** must be removed from input features because provide information about the target variable (i.e., they have values only if target is survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa15cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55714b93",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Data pre-processing\n",
    "\n",
    "Firstly, you will perform the pre-processing of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10951850",
   "metadata": {},
   "source": [
    "### 2.1 Train and Test splitting with Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59aaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d83667",
   "metadata": {},
   "source": [
    "The dataset is a slightly **imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbf9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8602dfd",
   "metadata": {},
   "source": [
    "#### Exercise 2.6.1\n",
    "\n",
    "Extract the input features in `X` and the target values in `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf209195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START CODE HERE (~2 lines) ####\n",
    "\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5139abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cfe15",
   "metadata": {},
   "source": [
    "#### Exercise 2.1.2\n",
    "\n",
    "Split the dataset into **train** and **test**. In this case, the dataset is **imbalance**. Therefore, it is recommended to split using stratification (i.e., the class label distribution will be preserved during the splitting).\n",
    "\n",
    "Split with 80% for training and 20% for validation. Shuffle the dataset before splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ac906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START CODE HERE (~1 line) ####\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(None, None, test_size=None, shuffle=None, random_state=42, stratify=None)\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fd9e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of training examples: {len(X_train)}\")\n",
    "print(f\"Number of testing examples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585257c9",
   "metadata": {},
   "source": [
    "### 2.2 Handling missing values\n",
    "\n",
    "#### Exercise 2.2.1\n",
    "Count the number of **null values** in training and test, and store them in the variables `nan_count_train` and `nan_count_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START CODE HERE (~2 lines) ####\n",
    "\n",
    "nan_count_train = None\n",
    "nan_count_test = None\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd85f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "print(nan_count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf3a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test\")\n",
    "print(nan_count_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f232acc",
   "metadata": {},
   "source": [
    "Sometimes, the **missing values** are not in the *nan* format.\n",
    "\n",
    "The next cell prints the format of *nan* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data types of missing values')\n",
    "for col in X_train.columns[X_train.isnull().any()]:\n",
    "    print(col, X_train[col][X_train[col].isnull()].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218efb75",
   "metadata": {},
   "source": [
    "In this case, all *nan* values are in the *nan* format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3140197",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.2\n",
    "\n",
    "Fill **null values** in the column `age` with the **mean** of the column `age` in the training and test set. Please compute the mean only on the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53666d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of null values in Train before pre-processing: {X_train.age.isnull().sum()}/{len(X_train)}')\n",
    "print(f'Number of null values in Test before pre-processing: {X_test.age.isnull().sum()}/{len(X_test)}')\n",
    "\n",
    "#### START CODE HERE (~2 lines) ####\n",
    "\n",
    "None\n",
    "None\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "print(f'Number of null values in Train after pre-processing: {X_train.age.isnull().sum()}/{len(X_train)}')\n",
    "print(f'Number of null values in Test after pre-processing: {X_test.age.isnull().sum()}/{len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd025603",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.3\n",
    "\n",
    "Fill **null values** in the column `fare` with the **median** of the column `fare` in the training and test set. Please compute the median only on the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of null values in Train before pre-processing: {X_train.fare.isnull().sum()}/{len(X_train)}')\n",
    "print(f'Number of null values in Test before pre-processing: {X_test.fare.isnull().sum()}/{len(X_test)}')\n",
    "\n",
    "#### START CODE HERE (~2 lines) ####\n",
    "\n",
    "None\n",
    "None\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "print(f'Number of null values in Train after pre-processing: {X_train.fare.isnull().sum()}/{len(X_train)}')\n",
    "print(f'Number of null values in Test after pre-processing: {X_test.fare.isnull().sum()}/{len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a0c04",
   "metadata": {},
   "source": [
    "#### Exercise 2.2.4\n",
    "\n",
    "Fill **null values** in the column `embarked` with the **most frequent value** of the column `embarked`. Please compute the most frequent only on the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of null values in Train before pre-processing: {X_train.embarked.isnull().sum()}/{len(X_train)}')\n",
    "print(f'Number of null values in Test before pre-processing: {X_test.embarked.isnull().sum()}/{len(X_test)}')\n",
    "\n",
    "#### START CODE HERE (~3 lines) ####\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=None)\n",
    "X_train['embarked'] = None\n",
    "X_test['embarked'] = None\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "print(f'Number of null values in Train after pre-processing: {X_train.embarked.isnull().sum()}/{len(X_train)}')\n",
    "print(f'Number of null values in Test after pre-processing: {X_test.embarked.isnull().sum()}/{len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7beda13",
   "metadata": {},
   "source": [
    "### 2.3 Features selection\n",
    "\n",
    "#### Exercise 2.3.1\n",
    "Remove columns *cabin*, *body*, *boat*, and *home.dest* from the train and test sets because they contain info about the target variable (i.e., the model could \"cheat\" predicting the target label based on the info in these attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ee6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START CODE HERE (~2 lines) ####\n",
    "\n",
    "X_train = None\n",
    "X_test = None\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632e529",
   "metadata": {},
   "source": [
    "#### Exercise 2.3.2\n",
    "\n",
    "Remove other columns that you think are useless features in predicting which people were more likely to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29669acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START CODE HERE (~2 lines) ####\n",
    "\n",
    "X_train = None\n",
    "X_test = None\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27452e71",
   "metadata": {},
   "source": [
    "The next cell plots the **correlation heatmap** using `Seaborn` and `df.corr()`. You will probably need to install `Seaborn` using the command `pip install seaborn`, and then restart your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "df_show = pd.concat([X_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare']], y_train], axis = 1)\n",
    "\n",
    "g = sns.heatmap(df_show.corr(),\n",
    "                annot=True, \n",
    "                cmap = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08db87",
   "metadata": {},
   "source": [
    "### 2.4 Features engineering (optional)\n",
    "\n",
    "#### Exercise 2.4.1\n",
    "\n",
    "If you want, you can create new columns here from the ones available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f56690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START CODE HERE ####\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd175c7",
   "metadata": {},
   "source": [
    "### 2.5 Discretization\n",
    "\n",
    "The next cell performs the **discretization** of the age column with **fixed-intervals**. \n",
    "You can learn more about **discretization** <a href=\"https://trainindata.medium.com/variable-discretization-in-machine-learning-7b09009915c2\" >here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_category = ['Child (0-14]', 'Young (14-24]', 'Adults (24-50]', 'Senior (50+]']\n",
    " \n",
    "X_train['age_disc']=pd.cut(x=X_train['age'], bins=[0,14,24,50,100],labels=age_category)\n",
    "X_train = X_train.drop(columns=['age']) # Remove the old age column\n",
    "\n",
    "X_test['age_disc']=pd.cut(x=X_test['age'], bins=[0,14,24,50,100],labels=age_category)\n",
    "X_test = X_test.drop(columns=['age']) # Remove the old age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48010f3",
   "metadata": {},
   "source": [
    "### 2.7 One-hot encoding\n",
    "\n",
    "The following cells perform the **one-hot encoding** of the categorical features using the `OneHotEncoder` of the **Scikit-Learn** library. You can also use a similar approach using the `get_dummies` function of **Pandas**.\n",
    "\n",
    "You can learn the differences between `OneHotEncoder` and `get_dummies` <a href=\"https://pythonsimplified.com/difference-between-onehotencoder-and-get_dummies/\" >here</a>.\n",
    "\n",
    "When building the `OneHotEncoder` object, the `handle_unknown` parameter is set to `'ignore'`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd50669",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ba927",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex', 'embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8360e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.fit(X_train[categorical_columns]) # Fit on training data\n",
    "\n",
    "temp_df = pd.DataFrame(data=ohe.transform(X_train[categorical_columns]).toarray(), \n",
    "                       columns=ohe.get_feature_names_out()) # Create a new DataFrame with only the one-hot encoded cols\n",
    "\n",
    "X_train.drop(columns=categorical_columns, axis=1, inplace=True) # Remove the old categorical columns from the original data\n",
    "X_train = pd.concat([X_train.reset_index(drop=True), temp_df], axis=1)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(data=ohe.transform(X_test[categorical_columns]).toarray(), \n",
    "                       columns=ohe.get_feature_names_out()) # Not fit on test data!\n",
    "\n",
    "X_test.drop(columns=categorical_columns, axis=1, inplace=True)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), temp_df], axis=1)\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38862c",
   "metadata": {},
   "source": [
    "### 2.7 Ordinal Encoding\n",
    "\n",
    "When the categorical feature are ordinal we can use ordinal Encoding. Since the order among the categories is important, encoding should reflect the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5478c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6500cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ord_enc = OrdinalEncoder(categories=[age_category]) # Should be a list becuause you can specify the categories for multiple columns\n",
    "\n",
    "\n",
    "ord_enc.fit(X_train.loc[:, [\"age_disc\"]]) # Fit on training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"age_disc_enc\"] = ord_enc.transform(X_train.loc[:, [\"age_disc\"]])\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=[\"age_disc\"], axis=1, inplace=True)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"age_disc_enc\"] = ord_enc.transform(X_test.loc[:, [\"age_disc\"]])\n",
    "X_test.drop(columns=[\"age_disc\"], axis=1, inplace=True)\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4aae5",
   "metadata": {},
   "source": [
    "### 2.8 Normalization/Standardization\n",
    "\n",
    "#### Exercise 2.8.1 \n",
    "\n",
    "Perform **Min-Max** normalization of the *numerical features*. Remember to **fit** on the training and not on the test. Note that `age_disc_enc` in this case is categorical but can be normalized too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b41b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numerical_features = [\"pclass\", \"sibsp\", \"parch\", \"fare\", \"age_disc_enc\"]\n",
    "\n",
    "#### START CODE HERE (~4 lines) ####\n",
    "\n",
    "minmax_s = None\n",
    "\n",
    "minmax_s.fit(None) \n",
    "\n",
    "X_train[numerical_features] = None\n",
    "X_test[numerical_features] = None\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32674ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7417ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ca9658",
   "metadata": {},
   "source": [
    "### 2.9 Features Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d98cc0",
   "metadata": {},
   "source": [
    "Now we fit PCA on the standardized training data and compute the cumulative explained variance. This tells us how much variance is captured as we increase the number of components.\n",
    "\n",
    "👉 Plot the cumulative variance curve and add a red line at 90% to guide component selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab013e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#start code\n",
    "\n",
    "\n",
    "#end code\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d244129",
   "metadata": {},
   "source": [
    "Based on the plot above, select the minimum number of components needed to reach a certain threshold (e.g. 90% of the total variance).\n",
    "\n",
    "👉 Compute the number of components programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "\n",
    "#start code\n",
    "\n",
    "#end code\n",
    "\n",
    "print(f\"Number of components to reach {threshold*100:.0f}% variance: {n_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf1409",
   "metadata": {},
   "source": [
    "Now we re-fit PCA using only the selected number of components, and project both training and test data into this reduced space.\n",
    "\n",
    "👉 Use .fit_transform() on training, and .transform() on test (no refit!).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start code\n",
    "\n",
    "\n",
    "\n",
    "#end code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37532d80",
   "metadata": {},
   "source": [
    "The loadings show how much each original feature contributes to each principal component.\n",
    "\n",
    "👉 Compute the loadings and visualize the contributions of each feature to PC₁ and PC₂ using a horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start code\n",
    "# as index use: features = [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "\n",
    "#end code\n",
    "\n",
    "plt.xlabel('Loadings')\n",
    "plt.ylabel('Feature')\n",
    "plt.title(f'Contribution of features to PC$_1$ and PC$_2$')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e619895",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Model Training and Evaluation\n",
    "\n",
    "Now, you can **train** and **evaluate** a **binary classification** model on the pre-processed dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b6c24",
   "metadata": {},
   "source": [
    "### 3.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade0de30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bb49094",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412dd32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4Engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
